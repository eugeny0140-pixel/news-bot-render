import os
import time
import logging
import re
import feedparser
import requests
import html
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
import cloudscraper
from deep_translator import GoogleTranslator
from supabase import create_client

# === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_IDS = [cid.strip() for cid in os.getenv("CHANNEL_ID1", "").split(",") if cid.strip()]
if os.getenv("CHANNEL_ID2"):
    CHANNEL_IDS.extend([cid.strip() for cid in os.getenv("CHANNEL_ID2").split(",") if cid.strip()])

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ ===
for var in ["TELEGRAM_BOT_TOKEN", "CHANNEL_ID1", "SUPABASE_URL", "SUPABASE_KEY"]:
    if not os.getenv(var):
        logger.error(f"‚ùå –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è {var} –Ω–µ –∑–∞–¥–∞–Ω–∞!")
        exit(1)

# === –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Supabase ===
try:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    supabase.table("published_articles").select("url").limit(1).execute()
    logger.info("‚úÖ Supabase –ø–æ–¥–∫–ª—é—á—ë–Ω")
except Exception as e:
    logger.error(f"‚ùå Supabase –æ—à–∏–±–∫–∞: {e}")
    exit(1)

# === –ò—Å—Ç–æ—á–Ω–∏–∫–∏ (–≤—Å–µ 19) ===
SOURCES = [
    # 1. Good Judgment (–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ superforecasting)
    {"name": "Good Judgment", "rss": "https://goodjudgment.com/blog/feed/", "method": "rss"},
    
    # 2. Johns Hopkins (–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π think-tank)
    {"name": "Johns Hopkins", "url": "https://www.centerforhealthsecurity.org/news/", "method": "html_parser"},
    
    # 3. Metaculus (–û–Ω–ª–∞–π–Ω-–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞)
    {"name": "Metaculus", "rss": "https://metaculus.com/feed/updates/", "method": "rss"},
    
    # 4. DNI Global Trends (–ì–æ—Å. think-tank)
    {"name": "DNI Global Trends", "url": "https://www.dni.gov/index.php/gt2040-home", "method": "html_parser"},
    
    # 5. RAND Corporation (Think-tank)
    {"name": "RAND", "rss": "https://www.rand.org/rss/recent.xml", "method": "rss"},
    
    # 6. World Economic Forum (Think-tank/—Ñ–æ—Ä—É–º)
    {"name": "World Economic Forum", "rss": "https://www.weforum.org/agenda/archive/feed", "method": "rss"},
    
    # 7. CSIS (Think-tank)
    {"name": "CSIS", "rss": "https://www.csis.org/rss.xml", "method": "rss"},
    
    # 8. Atlantic Council (Think-tank)
    {"name": "Atlantic Council", "rss": "https://www.atlanticcouncil.org/feed/", "method": "rss"},
    
    # 9. Chatham House (Think-tank)
    {"name": "Chatham House", "rss": "https://www.chathamhouse.org/feed", "method": "rss"},
    
    # 10. The Economist (–ñ—É—Ä–Ω–∞–ª)
    {"name": "ECONOMIST", "rss": "https://www.economist.com/the-world-this-week/rss.xml", "method": "rss"},
    
    # 11. Bloomberg (–û–Ω–ª–∞–π–Ω/broadcaster)
    {"name": "BLOOMBERG", "rss": "https://www.bloomberg.com/politics/feeds/site.xml", "method": "rss"},
    
    # 12. Reuters Institute (–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π/–æ–Ω–ª–∞–π–Ω)
    {"name": "Reuters Institute", "rss": "https://reutersinstitute.politics.ox.ac.uk/feed", "method": "rss"},
    
    # 13. Foreign Affairs (–ñ—É—Ä–Ω–∞–ª)
    {"name": "Foreign Affairs", "rss": "https://www.foreignaffairs.com/rss.xml", "method": "rss"},
    
    # 14. CFR (Think-tank)
    {"name": "CFR", "rss": "https://www.cfr.org/rss.xml", "method": "rss"},
    
    # 15. BBC Future (Broadcaster/–æ–Ω–ª–∞–π–Ω)
    {"name": "BBC Future", "rss": "http://feeds.bbci.co.uk/news/science_and_environment/rss.xml", "method": "rss"},
    
    # 16. Future Timeline (–ù–∏—à–µ–≤—ã–π –±–ª–æ–≥)
    {"name": "Future Timeline", "rss": "http://futuretimeline.net/blog.rss", "method": "rss_with_fallback"},
    
    # 17. Carnegie Endowment (Think-tank)
    {"name": "Carnegie", "url": "https://carnegieendowment.org/publications/", "method": "html_parser"},
    
    # 18. Bruegel (Think-tank)
    {"name": "Bruegel", "url": "https://www.bruegel.org/analysis", "method": "html_parser"},
    
    # 19. E3G (Think-tank)
    {"name": "E3G", "rss": "https://www.e3g.org/feed/", "method": "rss"},
]

# === –¢—Ä–µ—Ö—ç—Ç–∞–ø–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è –†–æ—Å—Å–∏–∏/–£–∫—Ä–∞–∏–Ω—ã, –°–í–û –∏ –∫—Ä–∏–ø—Ç–æ—Ä—ã–Ω–∫–∞ ===
# –≠–¢–ê–ü 1: –ü–û–ó–ò–¢–ò–í–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø
KEYWORDS = [
    # –ì–µ–æ–ø–æ–ª–∏—Ç–∏–∫–∞ –∏ –≤–æ–µ–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
    r"\b(russia|rus|—Ä–æ—Å—Å–∏–π—Å–∫(–∞—è|–æ–µ|–∏–π|–∏—Ö)|—Ä—Ñ|kremlin|putin|belarus|–±–µ–ª–∞—Ä—É—Å—å)\b",
    r"\b(ukraine|ukrainian|kiev|kyiv|zelensk(y|yy)|donbas|crimea|kherson|kharkiv|lviv)\b",
    r"\b(russian invasion|special military operation|SVO|russo-ukrainian war|ukraine conflict)\b",
    r"\b(russian military|wagner group|prigozhin|separatists|LNR|DNR|annexation)\b",
    r"\b(ukrainian forces|ATACMS|HIMARS|f-16|patriot system|counteroffensive)\b",
    r"\b(sanctions (against|on) russia|eu sanctions|price cap|SWIFT ban)\b",
    r"\b(iaea zaporizhzhia|nuclear plant|nord stream sabotage)\b",
    
    # –ö—Ä–∏–ø—Ç–æ—Ä—ã–Ω–æ–∫ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –†–§/–£–∫—Ä–∞–∏–Ω—ã
    r"\b(russia crypto|digital ruble|—Ü–∏—Ñ—Ä–æ–≤–æ–π —Ä—É–±–ª—å|cbr digital assets|garantex exchange)\b",
    r"\b(ukraine crypto donations|war bonds crypto|kuna exchange|come back alive crypto)\b",
    r"\b(russian crypto ban|cbr cryptocurrency regulation|rossvyaz crypto block)\b",
    r"\b(energy crypto mining russia|iran russia crypto|belarus crypto scheme)\b",
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è –∏ –∏–Ω—Å—Ç–∏—Ç—É—Ç—ã
    r"\b(ministry of defence ru|mod ru|rostec|alrosa|gazprom|rosneft)\b",
    r"\b(nato russia|finland nato|sweden nato|budapest memorandum)\b",
    r"\b(mobilization russia|filobank|shadow fleet|parallel imports russia)\b",
    r"\b(un vote russia|international court justice ukraine|icc putin)\b"
]

# –≠–¢–ê–ü 2: –ù–ï–ì–ê–¢–ò–í–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø (–ß–Å–†–ù–´–ô –°–ü–ò–°–û–ö)
BLACKLIST = [
    # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π –¥–ª—è "war"
    r"\bstar wars\b", r"\bworld of warcraft\b", r"\bwarhammer\b", r"\bwar of the roses\b",
    
    # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –æ–±—â–∏—Ö –∫—Ä–∏–ø—Ç–æ-–Ω–æ–≤–æ—Å—Ç–µ–π –±–µ–∑ —Å–≤—è–∑–∏ —Å –†–§/–£–∫—Ä
    r"\bbitcoin price\b.*\b(analysis|forecast|technical)\b", 
    r"\bethereum merge\b", r"\bcrypto etf approval\b", r"\bcoinbase earnings\b",
    
    # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø–∞–Ω–¥–µ–º–∏–π –±–µ–∑ –≥–µ–æ–ø—Ä–∏–≤—è–∑–∫–∏
    r"\bpandemic\b.*\b(flu|h5n1|mpox)\b", r"\bcovid-19\b.*\b(vaccine|variant)\b\s*[^.]*?\b(not|without)\b\s*\b(russia|ukraine)\b",
    
    # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –≤–æ–µ–Ω–Ω—ã—Ö —É—á–µ–Ω–∏–π –±–µ–∑ —Å–≤—è–∑–∏ —Å —Ä–µ–≥–∏–æ–Ω–æ–º
    r"\bmilitary exercise\b.*\b(nato|pacific|china|india)\b", 
    r"\bdrone show\b", r"\bnuclear safety\b.*\b(japan|fukushima)\b",
    
    # –°–∞–Ω–∫—Ü–∏–∏ –ø—Ä–æ—Ç–∏–≤ –¥—Ä—É–≥–∏—Ö —Å—Ç—Ä–∞–Ω
    r"\bsanction[s]?\b.*\b(venezuela|iran|north korea|myanmar|syria|belarus)\b",
    
    # –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –¥—Ä–æ–Ω—ã
    r"\bdrone delivery\b.*\b(amazon|google|wing)\b"
]

# –≠–¢–ê–ü 3: –ö–û–ù–¢–ï–ö–°–¢–ù–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø
CONTEXT_TERMS = r"\b(russia|ukraine|belarus|kremlin|putin|zelensk(y|yy)?|donbas|crimea|kyiv|kiev|moscow|russian|ukrainian|wagner|rostec|gazprom|LNR|DNR|ukrainian territory)\b"
CONTEXT_WINDOW = 200  # —Å–∏–º–≤–æ–ª–æ–≤ –≤ –∫–∞–∂–¥—É—é —Å—Ç–æ—Ä–æ–Ω—É –æ—Ç –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
CRITICAL_TERMS = [
    r"\b(?:war|attack|strike|sanction[s]?|military|conflict|drone|missile|rocket|bomb|nuclear|bio\w*)\b",
    r"\bcrypto(?:currency)?\b",
    r"\b(?:pandemic|virus|vaccine)\b"
]

def is_relevant(text: str) -> bool:
    """–¢—Ä—ë—Ö—ç—Ç–∞–ø–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π"""
    text_lower = text.lower()
    
    # === –≠–¢–ê–ü 1: –ü–û–ó–ò–¢–ò–í–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø ===
    keyword_matches = []
    for pattern in KEYWORDS:
        matches = list(re.finditer(pattern, text_lower, re.IGNORECASE | re.UNICODE))
        if matches:
            keyword_matches.extend(matches)
    
    if not keyword_matches:
        return False
    
    # === –≠–¢–ê–ü 2: –ù–ï–ì–ê–¢–ò–í–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø ===
    for pattern in BLACKLIST:
        if re.search(pattern, text_lower, re.IGNORECASE | re.UNICODE):
            return False
    
    # === –≠–¢–ê–ü 3: –ö–û–ù–¢–ï–ö–°–¢–ù–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø ===
    critical_compiled = re.compile("|".join(CRITICAL_TERMS), re.IGNORECASE | re.UNICODE)
    
    for match in keyword_matches:
        start, end = match.span()
        matched_text = match.group()
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —è–≤–Ω–æ –≥–µ–æ–ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã (–Ω–µ —Ç—Ä–µ–±—É—é—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        if re.search(r"(russia|ukraine|kremlin|putin|zelensk(y|yy)?|donbas|crimea|wagner|rostec)", matched_text, re.IGNORECASE):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å —Ç–µ—Ä–º–∏–Ω–∞
        if critical_compiled.search(matched_text):
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ
            context_start = max(0, start - CONTEXT_WINDOW)
            context_end = min(len(text_lower), end + CONTEXT_WINDOW)
            context_snippet = text_lower[context_start:context_end]
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            if re.search(CONTEXT_TERMS, context_snippet, re.IGNORECASE | re.UNICODE):
                return True
        else:
            # –ù–µ–∫—Ä–∏—Ç–∏—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, "digital ruble") –≤—Å–µ–≥–¥–∞ –ø—Ä–æ—Ö–æ–¥—è—Ç
            return True
    
    return False

# === –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===
def clean_html(raw: str) -> str:
    if not raw:
        return ""
    text = re.sub(r'<[^>]+>', '', raw)
    text = html.unescape(text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def translate(text: str) -> str:
    if not text.strip():
        return ""
    try:
        return GoogleTranslator(source='auto', target='ru').translate(text)
    except Exception as e:
        logger.warning(f"GoogleTranslate failed: {e}. Using original text.")
        return text

def is_article_sent(url: str) -> bool:
    try:
        resp = supabase.table("published_articles").select("url").eq("url", url).execute()
        return len(resp.data) > 0
    except Exception as e:
        logger.error(f"Supabase check error: {e}")
        return False

def mark_article_sent(url: str, title: str):
    try:
        supabase.table("published_articles").insert({"url": url, "title": title}).execute()
        logger.info(f"‚úÖ Saved: {url}")
    except Exception as e:
        logger.error(f"Supabase insert error: {e}")

def send_to_telegram(prefix: str, title: str, lead: str, url: str):
    try:
        title_ru = translate(title)
        lead_ru = translate(lead)
        message = f"<b>{prefix}</b>: {title_ru}\n\n{lead_ru}\n\n–ò—Å—Ç–æ—á–Ω–∏–∫: {url}"

        for ch in CHANNEL_IDS:
            resp = requests.post(
                f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage",
                json={
                    "chat_id": ch,
                    "text": message,
                    "parse_mode": "HTML",
                    "disable_web_page_preview": False
                },
                timeout=10
            )
            if resp.status_code == 200:
                logger.info(f"üì§ Sent: {title[:60]}...")
            else:
                logger.error(f"‚ùå TG error: {resp.status_code} {resp.text}")
    except Exception as e:
        logger.exception(f"Telegram send failed: {e}")

# === –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ ===
def fetch_rss_feed(url):
    """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ RSS-–ª–µ–Ω—Ç—ã"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/rss+xml, application/xml;q=0.9, */*;q=0.8'
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        feed = feedparser.parse(response.content)
        return feed
    except Exception as e:
        logger.error(f"RSS fetch error for {url}: {e}")
        return feedparser.FeedParserDict(entries=[])

def fetch_rss_with_fallback(url):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ RSS —Å —Ä–µ–∑–µ—Ä–≤–Ω—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
    try:
        return fetch_rss_feed(url)
    except Exception as e:
        logger.warning(f"RSS fallback error for {url}: {e}")
        return feedparser.FeedParserDict(entries=[])

def fetch_with_cloudscraper(url):
    """–û–±—Ö–æ–¥ –∑–∞—â–∏—Ç—ã Cloudflare —Å –ø–æ–º–æ—â—å—é cloudscraper"""
    try:
        scraper = cloudscraper.create_scraper()
        response = scraper.get(url, timeout=15)
        response.raise_for_status()
        return feedparser.parse(response.content)
    except Exception as e:
        logger.error(f"Cloudscraper error for {url}: {e}")
        return feedparser.FeedParserDict(entries=[])

def parse_johns_hopkins():
    """–ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ Johns Hopkins Center for Health Security"""
    url = "https://www.centerforhealthsecurity.org/news/"
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        entries = []
        # –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        for article in soup.select('.resource-item, .news-item, .list-item'):
            title_elem = article.select_one('h3 a, h2 a, .title a')
            if not title_elem:
                continue
                
            title = title_elem.get_text().strip()
            link = title_elem['href']
            # Ensure absolute URL
            if link.startswith('/'):
                link = 'https://www.centerforhealthsecurity.org' + link
            
            desc_elem = article.select_one('.summary, .excerpt, p')
            desc = desc_elem.get_text().strip() if desc_elem else ""
            
            date_elem = article.select_one('.date, time')
            pub_date = date_elem.get_text().strip() if date_elem else time.strftime("%a, %d %b %Y %H:%M:%S +0000", time.gmtime())
            
            entries.append({
                'title': title,
                'link': link,
                'summary': desc,
                'published': pub_date
            })
        
        feed = feedparser.FeedParserDict()
        feed.entries = entries
        return feed
    except Exception as e:
        logger.error(f"Johns Hopkins parsing error: {e}")
        return feedparser.FeedParserDict(entries=[])

def parse_dni_global_trends():
    """–ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ DNI Global Trends"""
    url = "https://www.dni.gov/index.php/gt2040-home"
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        entries = []
        # –ü–æ–∏—Å–∫ –æ—Ç—á–µ—Ç–æ–≤ –∏ –Ω–æ–≤–æ—Å—Ç–µ–π
        for item in soup.select('a[href*="global-trends"], a[href*="gt2040"], .content a'):
            title = item.get_text().strip()
            if len(title) < 10:  # Skip very short titles
                continue
                
            link = item['href']
            # Ensure absolute URL
            if link.startswith('/'):
                link = 'https://www.dni.gov' + link
            
            # Filter relevant content
            if any(keyword in title.lower() for keyword in ['global trends', 'gt2040', 'russia', 'ukraine', 'security', 'geopolitical']) or \
               any(keyword in link.lower() for keyword in ['global-trends', 'gt2040']):
                desc = f"Global Trends report: {title}. This analysis covers geopolitical trends and future scenarios."
                
                entries.append({
                    'title': title,
                    'link': link,
                    'summary': desc,
                    'published': time.strftime("%a, %d %b %Y %H:%M:%S +0000", time.gmtime())
                })
        
        feed = feedparser.FeedParserDict()
        feed.entries = entries
        return feed
    except Exception as e:
        logger.error(f"DNI Global Trends parsing error: {e}")
        return feedparser.FeedParserDict(entries=[])

def parse_carnegie():
    """–ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ Carnegie Endowment"""
    url = "https://carnegieendowment.org/publications/"
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        entries = []
        # –ü–æ–∏—Å–∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–π
        for article in soup.select('.publications-list .item, .publication-item, .article'):
            title_elem = article.select_one('.title a, h3 a, h2 a')
            if not title_elem:
                continue
                
            title = title_elem.get_text().strip()
            link = title_elem['href']
            # Ensure absolute URL
            if link.startswith('/'):
                link = 'https://carnegieendowment.org' + link
            
            desc_elem = article.select_one('.summary, .excerpt, .description, p')
            desc = desc_elem.get_text().strip() if desc_elem else ""
            
            date_elem = article.select_one('.date, time')
            date = date_elem.get_text().strip() if date_elem else time.strftime("%a, %d %b %Y %H:%M:%S +0000", time.gmtime())
            
            # Add region filter for Russia/Europe content
            if any(keyword in title.lower() or keyword in desc.lower() 
                   for keyword in ['russia', 'ukraine', 'moscow', 'kremlin', 'putin', 'eastern europe', 'eurasia']):
                entries.append({
                    'title': title,
                    'link': link,
                    'summary': desc,
                    'published': date
                })
        
        feed = feedparser.FeedParserDict()
        feed.entries = entries
        return feed
    except Exception as e:
        logger.error(f"Carnegie parsing error: {e}")
        return feedparser.FeedParserDict(entries=[])

def parse_bruegel():
    """–ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞ Bruegel"""
    url = "https://www.bruegel.org/analysis"
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        entries = []
        # –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π
        for article in soup.select('.post-item, .blog-item, article'):
            title_elem = article.select_one('h3 a, h2 a, .title a')
            if not title_elem:
                continue
                
            title = title_elem.get_text().strip()
            link = title_elem['href']
            # Ensure absolute URL
            if not link.startswith('http'):
                link = 'https://www.bruegel.org' + link
            
            desc_elem = article.select_one('.excerpt, .summary, .description, p')
            desc = desc_elem.get_text().strip() if desc_elem else ""
            
            date_elem = article.select_one('.date, time')
            date = date_elem.get_text().strip() if date_elem else time.strftime("%a, %d %b %Y %H:%M:%S +0000", time.gmtime())
            
            # Filter for relevant topics
            if any(keyword in title.lower() or keyword in desc.lower() 
                   for keyword in ['russia', 'ukraine', 'sanctions', 'energy security', 'europe', 'security', 'geopolitics', 'defense']):
                entries.append({
                    'title': title,
                    'link': link,
                    'summary': desc,
                    'published': date
                })
        
        feed = feedparser.FeedParserDict()
        feed.entries = entries
        return feed
    except Exception as e:
        logger.error(f"Bruegel parsing error: {e}")
        return feedparser.FeedParserDict(entries=[])

def fetch_and_process():
    logger.info("üì° Checking feeds...")
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=7)
    
    for src in SOURCES:
        try:
            logger.info(f"Fetching feed from {src['name']} (method: {src.get('method', 'unknown')})")
            feed = None
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
            if src.get('method') == 'rss':
                feed = fetch_rss_feed(src['rss'])
            elif src.get('method') == 'rss_with_fallback':
                feed = fetch_rss_with_fallback(src['rss'])
            elif src.get('method') == 'cloudscraper':
                feed = fetch_with_cloudscraper(src['rss'])
            elif src.get('method') == 'html_parser':
                if src['name'] == "Johns Hopkins":
                    feed = parse_johns_hopkins()
                elif src['name'] == "DNI Global Trends":
                    feed = parse_dni_global_trends()
                elif src['name'] == "Carnegie":
                    feed = parse_carnegie()
                elif src['name'] == "Bruegel":
                    feed = parse_bruegel()
                else:
                    feed = feedparser.FeedParserDict(entries=[])
            else:
                if 'rss' in src:
                    feed = fetch_rss_feed(src['rss'])
                elif 'url' in src:
                    # Default to html parser if no method specified but url exists
                    if src['name'] in ["Johns Hopkins", "DNI Global Trends", "Carnegie", "Bruegel"]:
                        if src['name'] == "Johns Hopkins":
                            feed = parse_johns_hopkins()
                        elif src['name'] == "DNI Global Trends":
                            feed = parse_dni_global_trends()
                        elif src['name'] == "Carnegie":
                            feed = parse_carnegie()
                        elif src['name'] == "Bruegel":
                            feed = parse_bruegel()
                    else:
                        feed = fetch_rss_feed(src['url'])  # Try as RSS
                else:
                    logger.warning(f"No valid URL or RSS for source: {src['name']}")
                    continue
            
            if not hasattr(feed, 'entries') or not feed.entries:
                logger.warning(f"‚ùå Empty or invalid feed from {src['name']}")
                continue

            for entry in feed.entries:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                pub_date = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    pub_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    pub_date = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                elif hasattr(entry, 'published') and entry.published:
                    try:
                        pub_date = datetime.strptime(entry.published, '%Y-%m-%d').replace(tzinfo=timezone.utc)
                    except:
                        pass
                
                # –ü—Ä–æ–ø—É—Å–∫ —Å—Ç–∞—Ä—ã—Ö —Å—Ç–∞—Ç–µ–π (—Å—Ç–∞—Ä—à–µ 7 –¥–Ω–µ–π)
                if pub_date is not None and pub_date < cutoff_date:
                    continue
                
                url = entry.get("link", "").strip()
                if not url or is_article_sent(url):
                    continue

                title = entry.get("title", "").strip()
                desc = (entry.get("summary") or entry.get("description") or "").strip()
                desc = clean_html(desc)
                if not title or not desc:
                    continue

                if not is_relevant(title + " " + desc):
                    continue

                lead = desc.split("\n")[0].split(". ")[0].strip()
                if not lead:
                    lead = desc[:150] + "..." if len(desc) > 150 else desc
                
                send_to_telegram(src["name"], title, lead, url)
                mark_article_sent(url, title)
                time.sleep(0.5)

        except Exception as e:
            logger.error(f"‚ùå Error on {src['name']}: {e}")

    logger.info("‚úÖ Feed check completed.")

# === –ó–∞–ø—É—Å–∫ ===
if __name__ == "__main__":
    logger.info("üöÄ Starting Russia Monitor Bot (Background Worker) with all 19 sources...")
    while True:
        fetch_and_process()
        logger.info("üí§ Sleeping for 10 minutes...")
        time.sleep(10 * 60)
