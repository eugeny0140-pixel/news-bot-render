import os
import time
import logging
import re
import feedparser
import requests
import html
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
from supabase import create_client
from deep_translator import GoogleTranslator, YandexTranslator

# === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
# –ù–æ–≤—ã–µ ID –∫–∞–Ω–∞–ª–æ–≤
CHANNEL_IDS = ["-1002923537056", "-1002914190770"]

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ ===
for var in ["TELEGRAM_BOT_TOKEN", "SUPABASE_URL", "SUPABASE_KEY"]:
    if not os.getenv(var):
        logger.error(f"‚ùå –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è {var} –Ω–µ –∑–∞–¥–∞–Ω–∞!")
        exit(1)

# === –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Supabase ===
try:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    supabase.table("published_articles").select("url").limit(1).execute()
    logger.info("‚úÖ Supabase –ø–æ–¥–∫–ª—é—á—ë–Ω")
except Exception as e:
    logger.error(f"‚ùå Supabase –æ—à–∏–±–∫–∞: {e}")
    exit(1)

# === –§–∏–ª—å—Ç—Ä—ã –¥–ª—è –†–æ—Å—Å–∏–∏, –°–í–û, –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç, –º–∏—Ä–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞ –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Ç–µ–Ω–¥–µ–Ω—Ü–∏–π ===
SIMPLE_KEYWORDS = [
    # –†–æ—Å—Å–∏—è –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã
    r"russia|—Ä–æ—Å—Å–∏–π(—Å–∫–∞—è|—Å–∫–æ–π|—Å–∫–æ–º—É|—Å–∫–∏—Ö|—Å–∫–∏–º–∏|—Å–∫—É—é|—Å–∫–∏–µ|—Å–∫–∏–º–∏|—Å–∫–æ–º—É|—Å–∫–∏–º–∏|—Å—Ç–≤—É|—Å—Ç–≤–∞|—Å—Ç–≤–æ–º|—Å—Ç–≤|—Å–∫–æ–º|—Å–∫–∏–º–∏|—Å–∫|—Å–∫–æ–º—É|—Å–∫–∏–π|—Å–∫–∏—Ö|—Å–∫–æ–µ|—Å–∫–∏–º–∏)\b",
    r"moscow|–º–æ—Å–∫–≤(–∞|—É|—ã|–æ–π|–µ|–∞–º–∏|–∞—Ö)\b",
    r"kremlin|–∫—Ä–µ–º–ª(—å|—è|—é|–µ–º|–µ|–∏)\b",
    r"putin|–ø—É—Ç–∏–Ω(–∞|—É|—ã–º|–µ|—ã|—Å–∫–∏—Ö)\b",
    r"russian|—Ä—É—Å—Å–∫(–∏–π|–æ–≥–æ|–æ–π|–∏—Ö|–∏–µ|–∏–º|–∏–º–∏|–æ–º—É|—ã–º–∏)\b",
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –≤–æ–µ–Ω–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è
    r"svo|—Å–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏(—è|–∏|–µ–π|—é|–µ–π|—è—Ö|—è–º–∏|—è–º)\b",
    r"special military operation\b",
    r"war|–≤–æ–π–Ω(–∞|—ã|–µ|—É|–æ–π|–∞–º|–∞–º–∏|–∞—Ö)\b",
    r"conflict|–∫–æ–Ω—Ñ–ª–∏–∫—Ç(–∞|—É|–æ–º|–µ|—ã|–æ–≤|–∞–º–∏|–∞—Ö)\b",
    r"ukraine|—É–∫—Ä–∞–∏–Ω(–∞|—ã|–µ|—É|–æ–π|–∞–º|–∞–º–∏|–∞—Ö)\b",
    r"donbas|–¥–æ–Ω–±–∞—Å(—Å|–∞|—É|–æ–º|—Å–µ|—Å—ã|—Å–æ–≤|—Å–∞–º–∏|—Å–∞—Ö)\b",
    r"crimea|–∫—Ä—ã–º(–∞|—É|–æ–º|–µ|—ã|–æ–≤|–∞–º–∏|–∞—Ö)\b",
    r"military|–≤–æ–µ–Ω(–Ω—ã–µ|–Ω–æ–π|–Ω—ã—Ö|–Ω–æ–º—É|–Ω—ã–º–∏|–Ω–∞—è|–Ω–æ–µ|–Ω—ã–π|–Ω—ã–º)\b",
    r"army|–∞—Ä–º–∏(—è|–∏|—é|–µ–π|–µ|–π|—è–º–∏|—è—Ö)\b",
    r"sanctions?|—Å–∞–Ω–∫—Ü–∏(–∏|–π|—è–º|—è–º–∏|—è—Ö|–µ–π|—è|—é)\b",
    r"nato|\s–Ω\s*–∞\s*—Ç\s*–æ\b",
    
    # –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã
    r"crypto|–∫—Ä–∏–ø—Ç–æ(–≤–∞–ª—é—Ç|–≤–∞–ª—é—Ç–∞|–≤–∞–ª—é—Ç—ã|–≤–∞–ª—é—Ç—É|–≤–∞–ª—é—Ç–æ–π|–≤–∞–ª—é—Ç–∞–º|–≤–∞–ª—é—Ç–∞–º–∏|–≤–∞–ª—é—Ç–∞—Ö|—Å–µ—Ç—å)\b",
    r"bitcoin|–±–∏—Ç–∫–æ–∏–Ω(–∞|—É|–æ–º|–µ|—ã|–æ–≤|–∞–º–∏|–∞—Ö)\b",
    r"btc|eth|ethereum\b",
    r"ruble|—Ä—É–±–ª—å|—Ä—É–±–ª—è|—Ä—É–±–ª–µ–π|—Ä—É–±–ª–µ–º|—Ä—É–±–ª—è–º|—Ä—É–±–ª—è–º–∏|—Ä—É–±–ª—è—Ö|—Ü–∏—Ñ—Ä–æ–≤–æ–π —Ä—É–±–ª—å\b",
    r"blockchain|–±–ª–æ–∫—á–µ–π–Ω(–∞|—É|–æ–º|–µ|—ã|–æ–≤|–∞–º–∏|–∞—Ö)\b",
    r"mining|–º–∞–π–Ω–∏–Ω–≥(–∞|—É|–æ–º|–µ|–∏|–æ–≤|–∞–º–∏|–∞—Ö)\b",
    
    # –ú–∏—Ä–æ–≤–æ–π —Ä—ã–Ω–æ–∫
    r"market|—Ä—ã–Ω–æ–∫(–∞|—É|–æ–º|–µ|–∏|–æ–≤|–∞–º–∏|–∞—Ö)\b",
    r"economy|—ç–∫–æ–Ω–æ–º–∏–∫(–∞|–∏|–µ|—É|–æ–π|–∞–º|–∞–º–∏|–∞—Ö)\b",
    r"finance|—Ñ–∏–Ω–∞–Ω—Å(—ã|–æ–≤|–∞–º|–∞–º–∏|–∞—Ö|–æ–≤–∞—è|–æ–≤—ã–π|–æ–≤–æ–µ|–æ–≤—ã–º–∏)\b",
    r"trade|—Ç–æ—Ä–≥–æ–≤–ª(—è|–∏|–µ–π|—é|–µ|—è–º|—è–º–∏|—è—Ö)\b",
    r"stock|—Ñ–æ–Ω–¥–æ–≤(—ã–π|–æ–≥–æ|–æ–º—É|—ã—Ö|–æ–º—É)\b",
    r"oil|gas|–Ω–µ—Ñ—Ç(—å|–∏|—å—é|—å—é|—é|—è–º–∏|—è—Ö)\b",
    r"energy|—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫(–∞|–∏|–µ|—É|–æ–π|–∞–º|–∞–º–∏|–∞—Ö)\b",
    
    # –ú–∏—Ä–æ–≤—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏
    r"geopolitic|–≥–µ–æ–ø–æ–ª–∏—Ç–∏–∫(–∞|–∏|–µ|—É|–æ–π|–∞–º|–∞–º–∏|–∞—Ö)\b",
    r"global|–≥–ª–æ–±–∞–ª—å–Ω(—ã–π|–æ–≥–æ|–æ–º—É|—ã—Ö|—ã—Ö|—ã–º|—ã–º–∏|–æ–º)\b",
    r"trends?|—Ç–µ–Ω–¥–µ–Ω—Ü–∏(–∏|–π|—è–º|—è–º–∏|—è—Ö|–µ–π|—è|—é)\b",
    r"climate|–∫–ª–∏–º–∞—Ç(–∞|—É|–æ–º|–µ|—ã|–æ–≤|–∞–º–∏|–∞—Ö)\b",
    r"ai|–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç|–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ\b",
    r"security|–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç(—å|–∏|—å—é|—é|—è—Ö|—è–º|—è–º–∏)\b",
    r"technology|—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏(–∏|–π|—è–º|—è–º–∏|—è—Ö|–µ–π|—è|—é)\b",
    r"innovation|–∏–Ω–Ω–æ–≤–∞—Ü–∏(–∏|–π|—è–º|—è–º–∏|—è—Ö|–µ–π|—è|—é)\b"
]

def is_relevant_simple(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è –†–æ—Å—Å–∏–∏, –°–í–û, –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç, –º–∏—Ä–æ–≤–æ–≥–æ —Ä—ã–Ω–∫–∞ –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Ç–µ–Ω–¥–µ–Ω—Ü–∏–π"""
    if not text or len(text) < 10:
        return False
    
    text_lower = text.lower()
    
    # –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    for pattern in SIMPLE_KEYWORDS:
        if re.search(pattern, text_lower, re.IGNORECASE):
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
            if re.search(r"star wars|world of warcraft|warhammer|war of the roses|drone show|drone delivery\b(amazon|google|wing)\b", text_lower, re.IGNORECASE):
                continue
            return True
    
    return False

def safe_translate(text: str) -> str:
    """–ù–∞–¥–µ–∂–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥ —Å —Ä–µ–∑–µ—Ä–≤–Ω—ã–º –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–º"""
    if not text.strip() or len(text) < 5:
        return text
    
    try:
        # –ü—Ä–æ–±—É–µ–º Google Translate
        translator = GoogleTranslator(source='auto', target='ru')
        return translator.translate(text)
    except Exception as e:
        logger.warning(f"GoogleTranslate failed: {e}. Trying Yandex.")
        try:
            # –†–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: Yandex Translate (–±–µ—Å–ø–ª–∞—Ç–Ω—ã–π)
            translator = YandexTranslator(source='auto', target='ru')
            return translator.translate(text)
        except Exception as e2:
            logger.warning(f"YandexTranslate also failed: {e2}. Using original text.")
            return text

# === –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===
def clean_html(raw: str) -> str:
    if not raw:
        return ""
    text = re.sub(r'<[^>]+>', '', raw)
    text = html.unescape(text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def is_article_sent(url: str) -> bool:
    try:
        resp = supabase.table("published_articles").select("url").eq("url", url).execute()
        return len(resp.data) > 0
    except Exception as e:
        logger.error(f"Supabase check error: {e}")
        return False

def mark_article_sent(url: str, title: str):
    try:
        supabase.table("published_articles").insert({"url": url, "title": title}).execute()
        logger.info(f"‚úÖ Saved: {url}")
    except Exception as e:
        logger.error(f"Supabase insert error: {e}")

def send_to_telegram(prefix: str, title: str, lead: str, url: str):
    try:
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–∏–π
        title_ru = safe_translate(title)
        lead_ru = safe_translate(lead)
        
        message = f"<b>{prefix}</b>: {title_ru}\n\n{lead_ru}\n\n–ò—Å—Ç–æ—á–Ω–∏–∫: {url}"
        for ch in CHANNEL_IDS:
            resp = requests.post(
                f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage",
                json={
                    "chat_id": ch,
                    "text": message,
                    "parse_mode": "HTML",
                    "disable_web_page_preview": False
                },
                timeout=10
            )
            if resp.status_code == 200:
                logger.info(f"üì§ Sent: {title[:60]}...")
            else:
                logger.error(f"‚ùå TG error: {resp.status_code} {resp.text}")
    except Exception as e:
        logger.exception(f"Telegram send failed: {e}")

def fetch_rss_feed(url):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ RSS-–ª–µ–Ω—Ç—ã"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/rss+xml, application/xml;q=0.9, */*;q=0.8'
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        feed = feedparser.parse(response.content)
        return feed
    except Exception as e:
        logger.error(f"RSS fetch error for {url}: {e}")
        return feedparser.FeedParserDict(entries=[])

def parse_html_feed(url, selectors):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä HTML"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        entries = []
        for item in soup.select(selectors['container']):
            title_elem = item.select_one(selectors['title'])
            if not title_elem:
                continue
                
            title = title_elem.get_text().strip()
            link = title_elem['href'] if 'href' in title_elem.attrs else ""
            if link.startswith('/'):
                link = '/'.join(url.split('/')[:3]) + link
            
            desc_elem = item.select_one(selectors['desc'])
            desc = desc_elem.get_text().strip() if desc_elem else ""
            
            date_elem = item.select_one(selectors['date'])
            pub_date_str = date_elem.get_text().strip() if date_elem else datetime.now(timezone.utc).strftime("%a, %d %b %Y %H:%M:%S +0000")
            
            entries.append({
                'title': title,
                'link': link,
                'summary': desc,
                'published': pub_date_str
            })
        
        feed = feedparser.FeedParserDict()
        feed.entries = entries
        return feed
    except Exception as e:
        logger.error(f"HTML parsing error for {url}: {e}")
        return feedparser.FeedParserDict(entries=[])

# === –ò—Å—Ç–æ—á–Ω–∏–∫–∏ (—Å–∞–º—ã–µ –Ω–∞–¥–µ–∂–Ω—ã–µ) ===
SOURCES = [
    # 1. World Economic Forum (–û—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫)
    {"name": "WEF", "url": "https://www.weforum.org/agenda/archive/feed", "method": "rss"},
    
    # 2. Good Judgment
    {"name": "Good Judgment", "url": "https://goodjudgment.com/feed/", "method": "rss"},
    
    # 3. RAND Corporation
    {"name": "RAND", "url": "https://www.rand.org/rss/recent.xml", "method": "rss"},
    
    # 4. CSIS
    {"name": "CSIS", "url": "https://www.csis.org/rss.xml", "method": "rss"},
    
    # 5. Atlantic Council
    {"name": "Atlantic Council", "url": "https://www.atlanticcouncil.org/feed/", "method": "rss"},
    
    # 6. Chatham House
    {"name": "Chatham House", "url": "https://www.chathamhouse.org/feed", "method": "rss"},
    
    # 7. The Economist
    {"name": "Economist", "url": "https://www.economist.com/the-world-this-week/rss.xml", "method": "rss"},
    
    # 8. Bloomberg
    {"name": "Bloomberg", "url": "https://feeds.bloomberg.com/politics/news.rss", "method": "rss"},
    
    # 9. Foreign Affairs
    {"name": "Foreign Affairs", "url": "https://www.foreignaffairs.com/rss.xml", "method": "rss"},
    
    # 10. CFR
    {"name": "CFR", "url": "https://www.cfr.org/rss.xml", "method": "rss"},
    
    # 11. Carnegie Endowment (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä)
    {"name": "Carnegie", "url": "https://carnegieendowment.org/publications/", 
     "method": "html", 
     "selectors": {
         "container": ".views-row",
         "title": ".views-field-title a",
         "desc": ".views-field-field-pub-excerpt .field-content",
         "date": ".views-field-field-pub-date .field-content"
     }},
    
    # 12. Bruegel (—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä)
    {"name": "Bruegel", "url": "https://www.bruegel.org/analysis", 
     "method": "html", 
     "selectors": {
         "container": ".post-item",
         "title": "h3 a",
         "desc": ".excerpt",
         "date": ".date"
     }}
]

def fetch_and_process():
    logger.info("üì° Checking feeds...")
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=3)  # –ë–µ—Ä–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è
    
    for src in SOURCES:
        try:
            logger.info(f"Fetching feed from {src['name']} (method: {src['method']})")
            feed = None
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            if src['method'] == 'rss':
                feed = fetch_rss_feed(src['url'])
            elif src['method'] == 'html' and 'selectors' in src:
                feed = parse_html_feed(src['url'], src['selectors'])
            else:
                feed = fetch_rss_feed(src['url'])
            
            if not hasattr(feed, 'entries') or not feed.entries:
                logger.warning(f"‚ùå Empty or invalid feed from {src['name']}")
                continue

            for entry in feed.entries:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                pub_date = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    pub_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    pub_date = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                elif hasattr(entry, 'published') and entry.published:
                    try:
                        pub_date = datetime.strptime(entry.published, '%a, %d %b %Y %H:%M:%S %z').astimezone(timezone.utc)
                    except ValueError:
                        try:
                            pub_date = datetime.strptime(entry.published, '%Y-%m-%dT%H:%M:%SZ').replace(tzinfo=timezone.utc)
                        except ValueError:
                            pub_date = datetime.now(timezone.utc)
                
                # –ü—Ä–æ–ø—É—Å–∫ —Å—Ç–∞—Ä—ã—Ö —Å—Ç–∞—Ç–µ–π
                if pub_date and pub_date < cutoff_date:
                    continue
                
                url = entry.get("link", "").strip()
                if not url or is_article_sent(url):
                    continue

                title = entry.get("title", "").strip()
                desc = (entry.get("summary") or entry.get("description", "")).strip()
                desc = clean_html(desc)
                if not title or not desc:
                    continue

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
                full_text = title + " " + desc
                if not is_relevant_simple(full_text):
                    continue

                lead = desc.split("\n")[0].split(". ")[0].strip()
                if not lead:
                    lead = desc[:120] + "..." if len(desc) > 120 else desc
                
                send_to_telegram(src["name"], title, lead, url)
                mark_article_sent(url, title)
                time.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ Telegram

        except Exception as e:
            logger.error(f"‚ùå Error on {src['name']}: {e}")

    logger.info("‚úÖ Feed check completed.")

# === –ó–∞–ø—É—Å–∫ ===
if __name__ == "__main__":
    logger.info("üöÄ Starting Russia Monitor Bot with translation...")
    logger.info("üîç Using simple keyword filters for Russia/Ukraine and global topics")
    logger.info(f"‚úÖ Sending translations to channels: {', '.join(CHANNEL_IDS)}")
    logger.info(f"‚è≥ Checking last 3 days of news from {len(SOURCES)} sources")
    
    while True:
        fetch_and_process()
        logger.info("üí§ Sleeping for 10 minutes...")
        time.sleep(50* 60)
