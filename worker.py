import os
import time
import logging
import re
import feedparser
import requests
import html
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
import json
import cloudscraper
from deep_translator import GoogleTranslator
from supabase import create_client
import random

# === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_IDS = [cid.strip() for cid in os.getenv("CHANNEL_ID1", "").split(",") if cid.strip()]
if os.getenv("CHANNEL_ID2"):
    CHANNEL_IDS.extend([cid.strip() for cid in os.getenv("CHANNEL_ID2").split(",") if cid.strip()])

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ ===
for var in ["TELEGRAM_BOT_TOKEN", "CHANNEL_ID1", "SUPABASE_URL", "SUPABASE_KEY"]:
    if not os.getenv(var):
        logger.error(f"‚ùå –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è {var} –Ω–µ –∑–∞–¥–∞–Ω–∞!")
        exit(1)

# === –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Supabase ===
try:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    supabase.table("published_articles").select("url").limit(1).execute()
    logger.info("‚úÖ Supabase –ø–æ–¥–∫–ª—é—á—ë–Ω")
except Exception as e:
    logger.error(f"‚ùå Supabase –æ—à–∏–±–∫–∞: {e}")
    exit(1)

# === –ò—Å—Ç–æ—á–Ω–∏–∫–∏ —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ ===
SOURCES = [
    # 1. Good Judgment (–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ superforecasting)
    {"name": "Good Judgment", "type": "rss", "url": "https://goodjudgment.com/feed/"},
    
    # 2. Johns Hopkins (–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π think-tank)
    {"name": "Johns Hopkins", "type": "rss", "url": "https://www.centerforhealthsecurity.org/feed.xml"},
    
    # 3. Metaculus (–û–Ω–ª–∞–π–Ω-–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞)
    {"name": "Metaculus", "type": "api", "url": "https://www.metaculus.com/api2/questions/?forecast_type= binary&status= open&page=1&limit=5"},
    
    # 4. DNI Global Trends (–ì–æ—Å. think-tank)
    {"name": "DNI Global Trends", "type": "html", "url": "https://www.dni.gov/index.php/gt2040-home", "parser": "dni_parser"},
    
    # 5. RAND Corporation (Think-tank)
    {"name": "RAND", "type": "rss", "url": "https://www.rand.org/rss/recent.xml"},
    
    # 6. World Economic Forum (Think-tank/—Ñ–æ—Ä—É–º)
    {"name": "WEF", "type": "rss", "url": "https://www.weforum.org/agenda/archive/rss"},
    
    # 7. CSIS (Think-tank)
    {"name": "CSIS", "type": "rss", "url": "https://www.csis.org/rss.xml"},
    
    # 8. Atlantic Council (Think-tank)
    {"name": "Atlantic Council", "type": "rss", "url": "https://www.atlanticcouncil.org/feed/"},
    
    # 9. Chatham House (Think-tank)
    {"name": "Chatham House", "type": "rss", "url": "https://www.chathamhouse.org/feed"},
    
    # 10. The Economist (–ñ—É—Ä–Ω–∞–ª)
    {"name": "ECONOMIST", "type": "rss", "url": "https://www.economist.com/the-world-this-week/rss.xml"},
    
    # 11. Bloomberg (–û–Ω–ª–∞–π–Ω/broadcaster)
    {"name": "BLOOMBERG", "type": "rss", "url": "https://www.bloomberg.com/politics/feeds/site.xml"},
    
    # 12. Reuters Institute (–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π/–æ–Ω–ª–∞–π–Ω)
    {"name": "Reuters Institute", "type": "rss", "url": "https://reutersinstitute.politics.ox.ac.uk/feed"},
    
    # 13. Foreign Affairs (–ñ—É—Ä–Ω–∞–ª)
    {"name": "Foreign Affairs", "type": "rss", "url": "https://www.foreignaffairs.com/rss.xml"},
    
    # 14. CFR (Think-tank)
    {"name": "CFR", "type": "rss", "url": "https://www.cfr.org/rss.xml"},
    
    # 15. BBC Future (Broadcaster/–æ–Ω–ª–∞–π–Ω)
    {"name": "BBC Future", "type": "rss", "url": "http://feeds.bbci.co.uk/news/science_and_environment/rss.xml"},
    
    # 16. Future Timeline (–ù–∏—à–µ–≤—ã–π –±–ª–æ–≥)
    {"name": "Future Timeline", "type": "rss", "url": "http://futuretimeline.net/blog.rss"},
    
    # 17. Carnegie Endowment (Think-tank)
    {"name": "Carnegie", "type": "html", "url": "https://carnegieendowment.org/publications/", "parser": "carnegie_parser"},
    
    # 18. Bruegel (Think-tank) - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–∞—â–∏—Ç—É Cloudflare
    {"name": "Bruegel", "type": "cloudflare", "url": "https://www.bruegel.org/"},
    
    # 19. E3G (Think-tank)
    {"name": "E3G", "type": "rss", "url": "https://www.e3g.org/feed/"}
]

# === –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ===
KEYWORDS = [
    r"\brussia\b", r"\brussian\b", r"\bputin\b", r"\bmoscow\b", r"\bkremlin\b",
    r"\bukraine\b", r"\bukrainian\b", r"\bzelensky\b", r"\bkyiv\b", r"\bkiev\b",
    r"\bcrimea\b", r"\bdonbas\b", r"\bsanction[s]?\b", r"\bgazprom\b",
    r"\bnord\s?stream\b", r"\bwagner\b", r"\blavrov\b", r"\bshoigu\b",
    r"\bmedvedev\b", r"\bpeskov\b", r"\bnato\b", r"\beuropa\b", r"\busa\b",
    r"\bsoviet\b", r"\bussr\b", r"\bpost\W?soviet\b",
    # === –°–í–û –∏ –í–æ–π–Ω–∞ ===
    r"\bsvo\b", r"\b—Å–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è\b", r"\bspecial military operation\b",
    r"\b–≤–æ–π–Ω–∞\b", r"\bwar\b", r"\bconflict\b", r"\b–∫–æ–Ω—Ñ–ª–∏–∫—Ç\b",
    r"\b–Ω–∞—Å—Ç—É–ø–ª–µ–Ω–∏–µ\b", r"\boffensive\b", r"\b–∞—Ç–∞–∫–∞\b", r"\battack\b",
    r"\b—É–¥–∞—Ä\b", r"\bstrike\b", r"\b–æ–±—Å—Ç—Ä–µ–ª\b", r"\bshelling\b",
    r"\b–¥—Ä–æ–Ω\b", r"\bdrone\b", r"\bmissile\b", r"\b—Ä–∞–∫–µ—Ç–∞\b",
    r"\b—ç—Å–∫–∞–ª–∞—Ü–∏—è\b", r"\bescalation\b", r"\b–º–æ–±–∏–ª–∏–∑–∞—Ü–∏—è\b", r"\bmobilization\b",
    r"\b—Ñ—Ä–æ–Ω—Ç\b", r"\bfrontline\b", r"\b–∑–∞—Ö–≤–∞—Ç\b", r"\bcapture\b",
    r"\b–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ\b", r"\bliberation\b", r"\b–±–æ–π\b", r"\bbattle\b",
    r"\b–ø–æ—Ç–µ—Ä–∏\b", r"\bcasualties\b", r"\b–ø–æ–≥–∏–±\b", r"\bkilled\b",
    r"\b—Ä–∞–Ω–µ–Ω\b", r"\binjured\b", r"\b–ø–ª–µ–Ω–Ω—ã–π\b", r"\bprisoner of war\b",
    r"\b–ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã\b", r"\btalks\b", r"\b–ø–µ—Ä–µ–º–∏—Ä–∏–µ\b", r"\bceasefire\b",
    r"\b—Å–∞–Ω–∫—Ü–∏–∏\b", r"\bsanctions\b", r"\b–æ—Ä—É–∂–∏–µ\b", r"\bweapons\b",
    r"\b–ø–æ—Å—Ç–∞–≤–∫–∏\b", r"\bsupplies\b", r"\bhimars\b", r"\batacms\b",
    r"\bhour ago\b", r"\b—á–∞—Å –Ω–∞–∑–∞–¥\b", r"\bminutos atr√°s\b", r"\bÂ∞èÊó∂Ââç\b",
    # === –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞ ===
    r"\bbitcoin\b", r"\bbtc\b", r"\b–±–∏—Ç–∫–æ–∏–Ω\b", r"\bÊØîÁâπÂ∏Å\b",
    r"\bethereum\b", r"\beth\b", r"\b—ç—Ñ–∏—Ä\b", r"\b‰ª•Â§™Âùä\b",
    r"\bbinance coin\b", r"\bbnb\b", r"\busdt\b", r"\btether\b",
    r"\bxrp\b", r"\bripple\b", r"\bcardano\b", r"\bada\b",
    r"\bsolana\b", r"\bsol\b", r"\bdoge\b", r"\bdogecoin\b",
    r"\bavalanche\b", r"\bavax\b", r"\bpolkadot\b", r"\bdot\b",
    r"\bchainlink\b", r"\blink\b", r"\btron\b", r"\btrx\b",
    r"\bcbdc\b", r"\bcentral bank digital currency\b", r"\b—Ü–∏—Ñ—Ä–æ–≤–æ–π —Ä—É–±–ª—å\b",
    r"\bdigital yuan\b", r"\beuro digital\b", r"\bdefi\b", r"\b–¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å—ã\b",
    r"\bnft\b", r"\bnon-fungible token\b", r"\bsec\b", r"\b—Ü–± —Ä—Ñ\b",
    r"\b—Ä–µ–≥—É–ª—è—Ü–∏—è\b", r"\bregulation\b", r"\b–∑–∞–ø—Ä–µ—Ç\b", r"\bban\b",
    r"\b–º–∞–π–Ω–∏–Ω–≥\b", r"\bmining\b", r"\bhalving\b", r"\b—Ö–∞–ª–≤–∏–Ω–≥\b",
    r"\b–≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å\b", r"\bvolatility\b", r"\bcrash\b", r"\b–∫—Ä–∞—Ö\b",
    r"\bÂàöÂàö\b", r"\bÿØŸÇÿßÿ¶ŸÇ ŸÖÿ∂ÿ™\b",
    # === –ü–∞–Ω–¥–µ–º–∏—è ===
    r"\bpandemic\b", r"\b–ø–∞–Ω–¥–µ–º–∏—è\b", r"\bÁñ´ÊÉÖ\b", r"\bÿ¨ÿßÿ¶ÿ≠ÿ©\b",
    r"\boutbreak\b", r"\b–≤—Å–ø—ã—à–∫–∞\b", r"\b—ç–ø–∏–¥–µ–º–∏—è\b", r"\bepidemic\b",
    r"\bvirus\b", r"\b–≤–∏—Ä—É—Å\b", r"\b–≤–∏—Ä—É—Å—ã\b", r"\bÂèòÂºÇÊ†™\b",
    r"\bvaccine\b", r"\b–≤–∞–∫—Ü–∏–Ω–∞\b", r"\bÁñ´Ëãó\b", r"\bŸÑŸÇÿßÿ≠\b",
    r"\bbooster\b", r"\b–±—É—Å—Ç–µ—Ä\b", r"\b—Ä–µ–≤–∞–∫—Ü–∏–Ω–∞—Ü–∏—è\b",
    r"\bquarantine\b", r"\b–∫–∞—Ä–∞–Ω—Ç–∏–Ω\b", r"\bÈöîÁ¶ª\b", r"\bÿ≠ÿ¨ÿ± ÿµÿ≠Ÿä\b",
    r"\blockdown\b", r"\b–ª–æ–∫–¥–∞—É–Ω\b", r"\bÂ∞ÅÈîÅ\b",
    r"\bmutation\b", r"\b–º—É—Ç–∞—Ü–∏—è\b", r"\bÂèòÂºÇ\b",
    r"\bstrain\b", r"\b—à—Ç–∞–º–º\b", r"\bomicron\b", r"\bdelta\b",
    r"\bbiosafety\b", r"\b–±–∏–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å\b", r"\bÁîüÁâ©ÂÆâÂÖ®\b",
    r"\blab leak\b", r"\b–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —É—Ç–µ—á–∫–∞\b", r"\bÂÆûÈ™åÂÆ§Ê≥ÑÊºè\b",
    r"\bgain of function\b", r"\b—É—Å–∏–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏\b",
    r"\bwho\b", r"\b–≤–æ–∑\b", r"\bcdc\b", r"\b—Ä–æ—Å–ø–æ—Ç—Ä–µ–±–Ω–∞–¥–∑–æ—Ä\b",
    r"\binfection rate\b", r"\b–∑–∞—Ä–∞–∑–Ω–æ—Å—Ç—å\b", r"\bÊ≠ª‰∫°Áéá\b",
    r"\bhospitalization\b", r"\b–≥–æ—Å–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è\b",
    r"\bŸÇÿ®ŸÑ ÿ≥ÿßÿπÿßÿ™\b", r"\bÂàöÂàöÊä•Âëä\b"
]

# === User agents –¥–ª—è —Ä–æ—Ç–∞—Ü–∏–∏ ===
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (iPad; CPU OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (Android 11; Mobile; rv:89.0) Gecko/89.0 Firefox/89.0"
]

# === –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===
def clean_html(raw: str) -> str:
    if not raw:
        return ""
    text = re.sub(r'<[^>]+>', '', raw)
    text = html.unescape(text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def get_random_user_agent():
    return random.choice(USER_AGENTS)

def get_headers():
    return {"User-Agent": get_random_user_agent()}

def translate(text: str) -> str:
    if not text.strip():
        return ""
    try:
        return GoogleTranslator(source='auto', target='ru').translate(text)
    except Exception as e:
        logger.warning(f"GoogleTranslate failed: {e}. Using original text.")
        return text

def is_relevant(title: str, desc: str) -> bool:
    text = (title + " " + desc).lower()
    return any(re.search(pattern, text) for pattern in KEYWORDS)

def is_article_sent(url: str) -> bool:
    try:
        resp = supabase.table("published_articles").select("url").eq("url", url).execute()
        return len(resp.data) > 0
    except Exception as e:
        logger.error(f"Supabase check error: {e}")
        return False

def mark_article_sent(url: str, title: str):
    try:
        supabase.table("published_articles").insert({"url": url, "title": title}).execute()
        logger.info(f"‚úÖ Saved: {url}")
    except Exception as e:
        logger.error(f"Supabase insert error: {e}")

def send_to_telegram(prefix: str, title: str, lead: str, url: str):
    try:
        title_ru = translate(title)
        lead_ru = translate(lead)
        message = f"<b>{prefix}</b>: {title_ru}\n\n{lead_ru}\n\n–ò—Å—Ç–æ—á–Ω–∏–∫: {url}"

        for ch in CHANNEL_IDS:
            resp = requests.post(
                f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage",
                json={
                    "chat_id": ch,
                    "text": message,
                    "parse_mode": "HTML"
                },
                timeout=10
            )
            if resp.status_code == 200:
                logger.info(f"üì§ Sent: {title[:60]}...")
            else:
                logger.error(f"‚ùå TG error: {resp.status_code} {resp.text}")
    except Exception as e:
        logger.exception(f"Telegram send failed: {e}")

# === –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –±–µ–∑ RSS ===
def parse_dni_global_trends(html_content):
    """–ü–∞—Ä—Å–∏—Ç DNI Global Trends"""
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        articles = []
        
        # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏ –ø–æ —Ä–∞–∑–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
        article_elements = soup.select('.article, .post, .publication, [class*="article"], [class*="post"], [class*="publication"]')
        
        for article in article_elements:
            title_elem = article.select_one('h1, h2, h3, h4, .title, .headline')
            link_elem = article.select_one('a')
            desc_elem = article.select_one('p, .description, .summary, .excerpt')
            
            if title_elem and link_elem and desc_elem:
                title = title_elem.get_text().strip()
                link = link_elem['href']
                if not link.startswith('http'):
                    link = f"https://www.dni.gov{link}"
                desc = desc_elem.get_text().strip()
                
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –¥–∞—Ç—É
                date_elem = article.select_one('.date, time, [class*="date"]')
                pub_date = None
                if date_elem:
                    try:
                        pub_date = datetime.strptime(date_elem.get_text().strip(), '%B %d, %Y').replace(tzinfo=timezone.utc)
                    except:
                        pass
                
                articles.append({
                    'title': title,
                    'link': link,
                    'description': desc,
                    'published': pub_date
                })
        
        return articles
    except Exception as e:
        logger.error(f"DNI parsing error: {e}")
        return []

def parse_carnegie_endowment(html_content):
    """–ü–∞—Ä—Å–∏—Ç Carnegie Endowment"""
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        articles = []
        
        # –°—Ç–∞—Ç—å–∏ –≤ Carnegie –∏–º–µ—é—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        item_elements = soup.select('.publication-item, .media-item, .featured-item')
        
        for item in item_elements:
            title_elem = item.select_one('.title a, h3 a, .headline a')
            desc_elem = item.select_one('.abstract, .description, .summary')
            date_elem = item.select_one('.date, time')
            
            if title_elem:
                title = title_elem.get_text().strip()
                link = title_elem['href']
                if not link.startswith('http'):
                    link = f"https://carnegieendowment.org{link}"
                
                desc = desc_elem.get_text().strip() if desc_elem else "No description"
                
                # –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã
                pub_date = None
                if date_elem:
                    try:
                        date_text = date_elem.get_text().strip()
                        pub_date = datetime.strptime(date_text, '%B %d, %Y').replace(tzinfo=timezone.utc)
                    except:
                        pass
                
                articles.append({
                    'title': title,
                    'link': link,
                    'description': desc,
                    'published': pub_date
                })
        
        return articles
    except Exception as e:
        logger.error(f"Carnegie parsing error: {e}")
        return []

def parse_bruegel_cloudflare():
    """–û–±—Ö–æ–¥–∏—Ç –∑–∞—â–∏—Ç—É Cloudflare –Ω–∞ Bruegel.org"""
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º cloudscraper –¥–ª—è –æ–±—Ö–æ–¥–∞ Cloudflare
        scraper = cloudscraper.create_scraper()
        response = scraper.get("https://www.bruegel.org/articles/feed/", headers=get_headers(), timeout=15)
        response.raise_for_status()
        
        # –ü–∞—Ä—Å–∏–º RSS-–ª–µ–Ω—Ç—É
        feed = feedparser.parse(response.content)
        return feed.entries
    except Exception as e:
        logger.error(f"Bruegel (Cloudflare) error: {e}")
        return []

def fetch_and_process():
    logger.info("üì° Checking feeds from all 19 sources...")
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=7)
    
    for src in SOURCES:
        try:
            logger.info(f"üîç Processing {src['name']} ({src['type']})")
            
            articles = []
            entries = []
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            if src['type'] == 'rss':
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π RSS
                feed = feedparser.parse(src['url'], agent=get_random_user_agent())
                entries = feed.entries
                
            elif src['type'] == 'api':
                # API-–∑–∞–ø—Ä–æ—Å—ã
                if src['name'] == 'Metaculus':
                    response = requests.get(src['url'], headers=get_headers(), timeout=15)
                    response.raise_for_status()
                    data = response.json()
                    
                    for item in data.get('results', []):
                        articles.append({
                            'title': item.get('title', ''),
                            'link': f"https://www.metaculus.com{item.get('page_url', '')}",
                            'description': item.get('description', ''),
                            'published': datetime.fromisoformat(item.get('created_at', '')[:-1]).replace(tzinfo=timezone.utc) if item.get('created_at') else None
                        })
            
            elif src['type'] == 'html':
                # –ü–∞—Ä—Å–∏–Ω–≥ HTML
                response = requests.get(src['url'], headers=get_headers(), timeout=15)
                response.raise_for_status()
                
                if src['parser'] == 'dni_parser':
                    articles = parse_dni_global_trends(response.text)
                elif src['parser'] == 'carnegie_parser':
                    articles = parse_carnegie_endowment(response.text)
            
            elif src['type'] == 'cloudflare':
                # –û–±—Ö–æ–¥ Cloudflare
                if src['name'] == 'Bruegel':
                    entries = parse_bruegel_cloudflare()
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∑–∞–ø–∏—Å–∏ –∏–∑ RSS –∏–ª–∏ API
            for entry in entries:
                url = entry.get("link", "").strip()
                if not url:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç—É
                pub_date = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    pub_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    pub_date = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª–∞ –ª–∏ —Å—Ç–∞—Ç—å—è —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –∏ –Ω–µ —Å–ª–∏—à–∫–æ–º –ª–∏ –æ–Ω–∞ —Å—Ç–∞—Ä–∞—è
                if (pub_date is not None and pub_date < cutoff_date) or is_article_sent(url):
                    continue
                
                title = entry.get("title", "").strip()
                desc = (entry.get("summary") or entry.get("description") or "").strip()
                desc = clean_html(desc)
                
                if not title or not desc:
                    continue
                
                if not is_relevant(title, desc):
                    continue
                
                lead = desc.split("\n")[0].split(". ")[0].strip()
                if not lead:
                    continue
                
                send_to_telegram(src['name'], title, lead, url)
                mark_article_sent(url, title)
                time.sleep(0.5)
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Ç–∞—Ç—å–∏ –∏–∑ HTML-–ø–∞—Ä—Å–∏–Ω–≥–∞
            for article in articles:
                url = article.get("link", "").strip()
                if not url:
                    continue
                
                pub_date = article.get("published")
                if (pub_date is not None and pub_date < cutoff_date) or is_article_sent(url):
                    continue
                
                title = article.get("title", "").strip()
                desc = article.get("description", "").strip()
                desc = clean_html(desc)
                
                if not title or not desc:
                    continue
                
                if not is_relevant(title, desc):
                    continue
                
                lead = desc.split("\n")[0].split(". ")[0].strip()
                if not lead:
                    continue
                
                send_to_telegram(src['name'], title, lead, url)
                mark_article_sent(url, title)
                time.sleep(0.5)
        
        except Exception as e:
            logger.error(f"‚ùå Error processing {src['name']}: {str(e)}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å —Å–µ—Ä–≤–µ—Ä—ã
        time.sleep(1)
    
    logger.info("‚úÖ All feeds processed completed.")

# === –ó–∞–ø—É—Å–∫ ===
if __name__ == "__main__":
    logger.info("üöÄ Starting Russia Monitor Bot with all 19 sources...")
    while True:
        fetch_and_process()
        logger.info("üí§ Sleeping for 10 minutes...")
        time.sleep(10 * 60)
