import os
import time
import logging
import re
import feedparser
import requests
import html
from datetime import datetime, timezone, timedelta
from bs4 import BeautifulSoup
import cloudscraper
from deep_translator import GoogleTranslator
from supabase import create_client

# === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_IDS = [cid.strip() for cid in os.getenv("CHANNEL_ID1", "").split(",") if cid.strip()]
if os.getenv("CHANNEL_ID2"):
    CHANNEL_IDS.extend([cid.strip() for cid in os.getenv("CHANNEL_ID2").split(",") if cid.strip()])

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ ===
required_vars = ["TELEGRAM_BOT_TOKEN", "CHANNEL_ID1", "SUPABASE_URL", "SUPABASE_KEY"]
missing_vars = [var for var in required_vars if not os.getenv(var)]
if missing_vars:
    logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: {', '.join(missing_vars)}")
    exit(1)

# === –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Supabase ===
try:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
    result = supabase.table("published_articles").select("url").limit(1).execute()
    if result.error:
        logger.warning(f"Supabase table 'published_articles' might be empty. Creating if needed.")
    logger.info("‚úÖ Supabase –ø–æ–¥–∫–ª—é—á—ë–Ω")
except Exception as e:
    logger.error(f"‚ùå Supabase –æ—à–∏–±–∫–∞: {e}")
    exit(1)

# === –í–°–ï 19 –ò–°–¢–û–ß–ù–ò–ö–û–í –° –ü–†–û–í–ï–†–ï–ù–ù–´–ú–ò RSS/–ü–ê–†–°–ï–†–ê–ú–ò ===
SOURCES = [
    # 1. Good Judgment (–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ superforecasting)
    {"name": "Good Judgment", "rss": "https://goodjudgment.com/blog/feed/", "parser": "rss"},
    
    # 2. Johns Hopkins (–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π think-tank)
    {"name": "Johns Hopkins", "url": "https://www.centerforhealthsecurity.org/news/", "parser": "johns_hopkins"},
    
    # 3. Metaculus (–û–Ω–ª–∞–π–Ω-–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞)
    {"name": "Metaculus", "rss": "https://metaculus.com/feed/updates/", "parser": "rss"},
    
    # 4. DNI Global Trends (–ì–æ—Å. think-tank)
    {"name": "DNI Global Trends", "url": "https://www.dni.gov/index.php/global-trends", "parser": "dni"},
    
    # 5. RAND Corporation (Think-tank)
    {"name": "RAND", "rss": "https://www.rand.org/rss.xml", "parser": "rss"},
    
    # 6. World Economic Forum (Think-tank/—Ñ–æ—Ä—É–º)
    {"name": "World Economic Forum", "rss": "https://www.weforum.org/agenda/archive/feed", "parser": "rss"},
    
    # 7. CSIS (Think-tank)
    {"name": "CSIS", "rss": "https://www.csis.org/rss.xml", "parser": "rss"},
    
    # 8. Atlantic Council (Think-tank)
    {"name": "Atlantic Council", "rss": "https://www.atlanticcouncil.org/feed/", "parser": "rss"},
    
    # 9. Chatham House (Think-tank)
    {"name": "Chatham House", "rss": "https://www.chathamhouse.org/feed", "parser": "rss"},
    
    # 10. The Economist (–ñ—É—Ä–Ω–∞–ª)
    {"name": "ECONOMIST", "rss": "https://www.economist.com/the-world-this-week/rss.xml", "parser": "rss"},
    
    # 11. Bloomberg (–û–Ω–ª–∞–π–Ω/broadcaster)
    {"name": "BLOOMBERG", "rss": "https://www.bloomberg.com/feed/politics", "parser": "rss"},
    
    # 12. Reuters Institute (–ê–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–π/–æ–Ω–ª–∞–π–Ω)
    {"name": "Reuters Institute", "rss": "https://reutersinstitute.politics.ox.ac.uk/feed", "parser": "rss"},
    
    # 13. Foreign Affairs (–ñ—É—Ä–Ω–∞–ª)
    {"name": "Foreign Affairs", "rss": "https://www.foreignaffairs.com/rss.xml", "parser": "rss"},
    
    # 14. CFR (Think-tank)
    {"name": "CFR", "rss": "https://www.cfr.org/rss.xml", "parser": "rss"},
    
    # 15. BBC Future (Broadcaster/–æ–Ω–ª–∞–π–Ω)
    {"name": "BBC Future", "rss": "http://feeds.bbci.co.uk/news/science_and_environment/rss.xml", "parser": "rss"},
    
    # 16. Future Timeline (–ù–∏—à–µ–≤—ã–π –±–ª–æ–≥)
    {"name": "Future Timeline", "rss": "http://futuretimeline.net/blog.rss", "parser": "rss"},
    
    # 17. Carnegie Endowment (Think-tank)
    {"name": "Carnegie", "url": "https://carnegieendowment.org/publications/", "parser": "carnegie"},
    
    # 18. Bruegel (Think-tank)
    {"name": "Bruegel", "search_url": "https://www.bruegel.org/search?search_term=russia", "parser": "bruegel"},
    
    # 19. E3G (Think-tank)
    {"name": "E3G", "rss": "https://www.e3g.org/feed/", "parser": "rss"},
]

# === –°–¢–†–û–ì–ò–ï –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê –¢–û–õ–¨–ö–û –ü–†–û –†–û–°–°–ò–Æ ===
RUSSIA_KEYWORDS = [
    r"\brussia\b", r"\brussian\b", r"\bputin\b", r"\bmoscow\b", r"\bkremlin\b",
    r"\bukraine\b", r"\bukrainian\b", r"\bzelensky\b", r"\bkyiv\b", r"\bkiev\b",
    r"\bcrimea\b", r"\bdonbas\b", r"\bsanction[s]?\b", r"\bgazprom\b",
    r"\bnord\s?stream\b", r"\bwagner\b", r"\blavrov\b", r"\bshoigu\b",
    r"\bmedvedev\b", r"\bpeskov\b", r"\brussian army\b",
    r"\bsvo\b", r"\b—Å–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è\b", r"\bspecial military operation\b",
    r"\b–≤–æ–π–Ω–∞\b", r"\bwar\b", r"\bconflict\b", r"\b–∫–æ–Ω—Ñ–ª–∏–∫—Ç\b",
    r"\b–Ω–∞—Å—Ç—É–ø–ª–µ–Ω–∏–µ\b", r"\boffensive\b", r"\b–∞—Ç–∞–∫–∞\b", r"\battack\b",
    r"\b—É–¥–∞—Ä\b", r"\bstrike\b", r"\b–æ–±—Å—Ç—Ä–µ–ª\b", r"\bshelling\b",
    r"\b–¥—Ä–æ–Ω\b", r"\bdrone\b", r"\bmissile\b", r"\b—Ä–∞–∫–µ—Ç–∞\b",
    r"\b—ç—Å–∫–∞–ª–∞—Ü–∏—è\b", r"\bescalation\b", r"\b–º–æ–±–∏–ª–∏–∑–∞—Ü–∏—è\b", r"\bmobilization\b",
    r"\b—Ñ—Ä–æ–Ω—Ç\b", r"\bfrontline\b", r"\b–∑–∞—Ö–≤–∞—Ç\b", r"\bcapture\b",
    r"\b–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ\b", r"\bliberation\b", r"\b–±–æ–π\b", r"\bbattle\b",
    r"\b–ø–æ—Ç–µ—Ä–∏\b", r"\bcasualties\b", r"\b–ø–æ–≥–∏–±\b", r"\bkilled\b",
    r"\b—Ä–∞–Ω–µ–Ω\b", r"\binjured\b", r"\b–ø–ª–µ–Ω–Ω—ã–π\b", r"\bprisoner of war\b",
    r"\b–ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã\b", r"\btalks\b", r"\b–ø–µ—Ä–µ–º–∏—Ä–∏–µ\b", r"\bceasefire\b",
    r"\b—Å–∞–Ω–∫—Ü–∏–∏\b", r"\bsanctions\b", r"\b–æ—Ä—É–∂–∏–µ\b", r"\bweapons\b",
    r"\b–ø–æ—Å—Ç–∞–≤–∫–∏\b", r"\bsupplies\b", r"\bhimars\b", r"\batacms\b",
]

def clean_html(raw: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç HTML-—Ç–µ–≥–∏ –∏ –¥–µ–∫–æ–¥–∏—Ä—É–µ—Ç HTML-—Å—É—â–Ω–æ—Å—Ç–∏."""
    if not raw:
        return ""
    # –£–¥–∞–ª—è–µ–º —Ç–µ–≥–∏
    text = re.sub(r'<[^>]+>', '', raw)
    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
    text = html.unescape(text)
    # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def translate(text: str) -> str:
    if not text.strip():
        return ""
    try:
        translator = GoogleTranslator(source='auto', target='ru')
        return translator.translate(text)
    except Exception as e:
        logger.warning(f"GoogleTranslate failed: {e}. Using original text.")
        return text

def is_about_russia(title: str, desc: str) -> bool:
    """–°—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –æ –†–æ—Å—Å–∏–∏ –≤ —Å—Ç–∞—Ç—å–µ"""
    text = (title + " " + desc).lower()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø—Ä–æ –†–æ—Å—Å–∏—é
    has_russia_keywords = any(re.search(pattern, text) for pattern in RUSSIA_KEYWORDS)
    
    if not has_russia_keywords:
        return False
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–†–æ—Å—Å–∏—è —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
    negative_context = [
        r"\bnot russia\b", r"\bnot russian\b", r"\brather than russia\b",
        r"\bcompared to russia\b", r"\bunlike russia\b", r"\bvs russia\b",
        r"\bcompared with russia\b", r"\bin contrast to russia\b",
        r"\bno russia\b", r"\bno russian\b", r"\bwithout russia\b"
    ]
    
    if any(re.search(pattern, text) for pattern in negative_context):
        logger.debug(f"‚ùå –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ (–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç): {title}")
        return False
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –Ω–∞—á–∞–ª–µ —Ç–µ–∫—Å—Ç–∞
    first_300_chars = text[:300].lower()
    has_keywords_in_beginning = any(re.search(pattern, first_300_chars) for pattern in RUSSIA_KEYWORDS)
    
    return has_keywords_in_beginning

def is_article_sent(url: str) -> bool:
    try:
        resp = supabase.table("published_articles").select("url").eq("url", url).execute()
        if resp.error:
            logger.error(f"Supabase error checking URL {url}: {resp.error}")
            return False
        return len(resp.data) > 0
    except Exception as e:
        logger.error(f"Supabase check error for {url}: {e}")
        return False

def mark_article_sent(url: str, title: str):
    try:
        resp = supabase.table("published_articles").insert({"url": url, "title": title}).execute()
        if resp.error:
            logger.error(f"Supabase error inserting {url}: {resp.error}")
        else:
            logger.info(f"‚úÖ Saved: {url}")
    except Exception as e:
        logger.error(f"Supabase insert error: {e}")

def send_to_telegram(prefix: str, title: str, lead: str, url: str):
    try:
        title_ru = translate(title)
        lead_ru = translate(lead)
        message = f"<b>{prefix}</b>: {title_ru}\n\n{lead_ru}\n\n–ò—Å—Ç–æ—á–Ω–∏–∫: {url}"

        for ch in CHANNEL_IDS:
            resp = requests.post(
                f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage",
                json={
                    "chat_id": ch,
                    "text": message,
                    "parse_mode": "HTML"
                },
                timeout=10
            )
            if resp.status_code == 200:
                logger.info(f"üì§ Sent: {title[:60]}...")
            else:
                logger.error(f"‚ùå TG error: {resp.status_code} - {resp.text[:200]}")
    except Exception as e:
        logger.exception(f"Telegram send failed: {e}")

# === –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä—Å–µ—Ä—ã –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ ===
def parse_johns_hopkins():
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Johns Hopkins Center for Health Security"""
    url = "https://www.centerforhealthsecurity.org/news/"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        entries = []
        # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ç–µ–∫—É—â–µ–π –≤–µ—Ä—Å—Ç–∫–∏
        for article in soup.select('.item.news'):
            title_elem = article.select_one('h3.title a')
            if not title_elem:
                continue
                
            title = title_elem.get_text().strip()
            link = "https://www.centerforhealthsecurity.org" + title_elem['href']
            desc_elem = article.select_one('.description')
            desc = desc_elem.get_text().strip() if desc_elem else ""
            date_elem = article.select_one('.date')
            pub_date = date_elem.get_text().strip() if date_elem else ""
            
            entries.append({
                'title': title,
                'link': link,
                'summary': desc,
                'published': pub_date
            })
        
        feed = feedparser.FeedParserDict(entries=entries)
        return feed
    except Exception as e:
        logger.error(f"Johns Hopkins parsing error: {e}")
        return feedparser.FeedParserDict(entries=[])

def parse_dni():
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è DNI Global Trends"""
    url = "https://www.dni.gov/index.php/global-trends"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        entries = []
        # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –†–æ—Å—Å–∏–µ–π
        for article in soup.select('.item'):
            title_elem = article.select_one('h3 a')
            if not title_elem:
                continue
                
            title = title_elem.get_text().strip()
            link = "https://www.dni.gov" + title_elem['href'] if not title_elem['href'].startswith('http') else title_elem['href']
            desc_elem = article.select_one('.description')
            desc = desc_elem.get_text().strip() if desc_elem else ""
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç—å–∏ –ø—Ä–æ –†–æ—Å—Å–∏—é
            if "russia" in title.lower() or "russia" in desc.lower() or "putin" in title.lower():
                entries.append({
                    'title': title,
                    'link': link,
                    'summary': desc,
                    'published': time.strftime("%Y-%m-%d")
                })
        
        feed = feedparser.FeedParserDict(entries=entries)
        return feed
    except Exception as e:
        logger.error(f"DNI parsing error: {e}")
        return feedparser.FeedParserDict(entries=[])

def parse_carnegie():
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Carnegie Endowment"""
    url = "https://carnegieendowment.org/publications/"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        entries = []
        for article in soup.select('.publications-list .item'):
            title_elem = article.select_one('.title a')
            if not title_elem:
                continue
                
            title = title_elem.get_text().strip()
            link = "https://carnegieendowment.org" + title_elem['href'] if not title_elem['href'].startswith('http') else title_elem['href']
            desc_elem = article.select_one('.summary')
            desc = desc_elem.get_text().strip() if desc_elem else ""
            date_elem = article.select_one('.date')
            date = date_elem.get_text().strip() if date_elem else ""
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ç—å–∏ –ø—Ä–æ –†–æ—Å—Å–∏—é
            if "russia" in title.lower() or "russia" in desc.lower() or "putin" in title.lower() or "ukraine" in title.lower():
                entries.append({
                    'title': title,
                    'link': link,
                    'summary': desc,
                    'published': date
                })
        
        feed = feedparser.FeedParserDict(entries=entries)
        return feed
    except Exception as e:
        logger.error(f"Carnegie parsing error: {e}")
        return feedparser.FeedParserDict(entries=[])

def parse_bruegel():
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Bruegel (–æ–±—Ö–æ–¥ Cloudflare)"""
    url = "https://www.bruegel.org/search?search_term=russia"
    scraper = cloudscraper.create_scraper()
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    try:
        response = scraper.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        entries = []
        for result in soup.select('.search-result'):
            title_elem = result.select_one('.search-result__title a')
            if not title_elem:
                continue
                
            title = title_elem.get_text().strip()
            link = title_elem['href']
            if not link.startswith('http'):
                link = "https://www.bruegel.org" + link
                
            desc_elem = result.select_one('.search-result__summary')
            desc = desc_elem.get_text().strip() if desc_elem else ""
            
            entries.append({
                'title': title,
                'link': link,
                'summary': desc,
                'published': time.strftime("%Y-%m-%d")
            })
        
        feed = feedparser.FeedParserDict(entries=entries)
        return feed
    except Exception as e:
        logger.error(f"Bruegel parsing error: {e}")
        return feedparser.FeedParserDict(entries=[])

def fetch_feed(source):
    """–û–±—â–∏–π –º–µ—Ç–æ–¥ –ø–æ–ª—É—á–µ–Ω–∏—è —Ñ–∏–¥–∞ —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
    try:
        if source['parser'] == 'rss':
            url = source['rss']
            feed = feedparser.parse(url)
            if hasattr(feed, 'bozo') and feed.bozo:
                logger.warning(f"RSS feed warning for {source['name']}: {feed.bozo_exception}")
            return feed
        elif source['parser'] == 'johns_hopkins':
            return parse_johns_hopkins()
        elif source['parser'] == 'dni':
            return parse_dni()
        elif source['parser'] == 'carnegie':
            return parse_carnegie()
        elif source['parser'] == 'bruegel':
            return parse_bruegel()
        else:
            logger.warning(f"Unknown parser type {source['parser']} for {source['name']}")
            return feedparser.FeedParserDict(entries=[])
    except Exception as e:
        logger.error(f"Error fetching {source['name']}: {e}")
        return feedparser.FeedParserDict(entries=[])

def fetch_and_process():
    logger.info("üì° Checking feeds from all 19 sources...")
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=7)
    processed_count = 0
    sent_count = 0
    
    for src in SOURCES:
        try:
            logger.info(f"üîç Processing {src['name']} with {src['parser']} parser")
            feed = fetch_feed(src)
            
            if not hasattr(feed, 'entries') or not feed.entries:
                logger.warning(f"‚ùå No entries found for {src['name']}")
                continue

            for entry in feed.entries:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                pub_date = None
                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    pub_date = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
                elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                    pub_date = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
                elif hasattr(entry, 'published') and entry.published:
                    try:
                        pub_date = datetime.strptime(entry.published[:10], '%Y-%m-%d').replace(tzinfo=timezone.utc)
                    except:
                        pass
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ä—ã–µ —Å—Ç–∞—Ç—å–∏
                if pub_date is not None and pub_date < cutoff_date:
                    continue
                
                url = entry.get("link", "").strip()
                if not url:
                    continue
                    
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
                if is_article_sent(url):
                    continue

                title = entry.get("title", "").strip()
                desc = (entry.get("summary") or entry.get("description") or "").strip()
                desc = clean_html(desc)
                if not title or not desc:
                    continue

                # –°–¢–†–û–ì–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –¢–û–õ–¨–ö–û –ü–†–û –†–û–°–°–ò–Æ
                if not is_about_russia(title, desc):
                    logger.debug(f"üö´ Skipped (not about Russia): {title}")
                    continue

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ª–∏–¥ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
                lead = desc.split("\n")[0].split(". ")[0].strip()
                if not lead:
                    continue

                send_to_telegram(src["name"], title, lead, url)
                mark_article_sent(url, title)
                sent_count += 1
                time.sleep(0.5)
                processed_count += 1

        except Exception as e:
            logger.error(f"‚ùå Error processing {src['name']}: {e}")

    logger.info(f"‚úÖ Feed check completed for all sources. Processed: {processed_count} articles, Sent: {sent_count} articles.")

# === –ó–∞–ø—É—Å–∫ ===
if __name__ == "__main__":
    logger.info("üöÄ Starting Russia Monitor Bot with all 19 sources...")
    while True:
        try:
            fetch_and_process()
        except Exception as e:
            logger.exception(f"üî• CRITICAL ERROR in main loop: {e}")
        logger.info("üí§ Sleeping for 10 minutes...")
        time.sleep(10 * 60)
