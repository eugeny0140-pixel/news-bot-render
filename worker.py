import os
import time
import logging
import re
import feedparser
import requests
import html  # –î–æ–±–∞–≤–ª–µ–Ω –¥–ª—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è HTML-—Å—É—â–Ω–æ—Å—Ç–µ–π
from deep_translator import GoogleTranslator, MyMemoryTranslator
from supabase import create_client

# === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
TELEGRAM_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
CHANNEL_IDS = [cid.strip() for cid in os.getenv("CHANNEL_ID1", "").split(",") if cid.strip()]
if os.getenv("CHANNEL_ID2"):
    CHANNEL_IDS.extend([cid.strip() for cid in os.getenv("CHANNEL_ID2").split(",") if cid.strip()])

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ ===
for var in ["TELEGRAM_BOT_TOKEN", "CHANNEL_ID1", "SUPABASE_URL", "SUPABASE_KEY"]:
    if not os.getenv(var):
        logger.error(f"‚ùå –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è {var} –Ω–µ –∑–∞–¥–∞–Ω–∞!")
        exit(1)

# === –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Supabase ===
try:
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    supabase.table("published_articles").select("url").limit(1).execute()
    logger.info("‚úÖ Supabase –ø–æ–¥–∫–ª—é—á—ë–Ω")
except Exception as e:
    logger.error(f"‚ùå Supabase –æ—à–∏–±–∫–∞: {e}")
    exit(1)

# === –ò—Å—Ç–æ—á–Ω–∏–∫–∏ (—Å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –ø—Ä–µ—Ñ–∏–∫—Å–∞–º–∏) ===
# –£–±—Ä–∞–Ω—ã Bruegel –∏ Carnegie –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫
SOURCES = [
    {"name": "E3G", "rss": "https://www.e3g.org/feed/"},
    {"name": "Foreign Affairs", "rss": "https://www.foreignaffairs.com/rss.xml"},
    {"name": "Reuters Institute", "rss": "https://reutersinstitute.politics.ox.ac.uk/feed"},
    # {"name": "Bruegel", "rss": "https://www.bruegel.org/rss"}, # –ó–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –∏–∑-–∑–∞ –∑–∞—â–∏—Ç—ã
    {"name": "Chatham House", "rss": "https://www.chathamhouse.org/feed"},
    {"name": "CSIS", "rss": "https://www.csis.org/rss.xml"},
    {"name": "Atlantic Council", "rss": "https://www.atlanticcouncil.org/feed/"},
    {"name": "RAND", "rss": "https://www.rand.org/rss/recent.xml"},
    {"name": "CFR", "rss": "https://www.cfr.org/rss.xml"},
    # {"name": "Carnegie", "rss": "https://carnegieendowment.org/rss"}, # –ó–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –∏–∑-–∑–∞ 404
    {"name": "ECONOMIST", "rss": "https://www.economist.com/rss/the_world_this_week_rss.xml"},
    {"name": "BLOOMBERG", "rss": "https://www.bloomberg.com/politics/feeds/site.xml"},
    # –î–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    {"name": "BBC Future", "rss": "http://feeds.bbci.co.uk/news/science_and_environment/rss.xml"},
    {"name": "Future Timeline", "rss": "http://futuretimeline.net/blog.rss"},
]

# === –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===
def clean_html(raw: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç HTML-—Ç–µ–≥–∏ –∏ –¥–µ–∫–æ–¥–∏—Ä—É–µ—Ç HTML-—Å—É—â–Ω–æ—Å—Ç–∏."""
    if not raw:
        return ""
    # –°–Ω–∞—á–∞–ª–∞ —É–¥–∞–ª—è–µ–º —Ç–µ–≥–∏
    text = re.sub(r'<[^>]+>', '', raw)
    # –ó–∞—Ç–µ–º –¥–µ–∫–æ–¥–∏—Ä—É–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ —Ç–∏–ø–∞ &nbsp; -> –ø—Ä–æ–±–µ–ª
    text = html.unescape(text)
    # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–±–µ–ª
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def translate(text: str) -> str:
    if not text.strip():
        return ""
    try:
        return GoogleTranslator(source='auto', target='ru').translate(text)
    except Exception as e:
        logger.warning(f"GoogleTranslate failed: {e}. Trying MyMemory.")
        try:
            return MyMemoryTranslator(source='auto', target='ru').translate(text)
        except:
            return text

def is_relevant(title: str, desc: str) -> bool:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç (–∑–∞–≥–æ–ª–æ–≤–æ–∫ + –æ–ø–∏—Å–∞–Ω–∏–µ) —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏.
    """
    text = (title + " " + desc).lower()
    # –ü—Ä–æ—Å—Ç—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ (–ø–æ–¥—Å—Ç—Ä–æ–∫–∏)
    keywords = [
        "russia", "russian", "putin", "moscow", "kremlin",
        "ukraine", "ukrainian", "zelensky", "kyiv", "kiev",
        "crimea", "donbas", "sanction", "gazprom",
        "nord stream", "wagner", "lavrov", "shoigu",
        "medvedev", "peskov", "nato", "europa", "usa",
        "soviet", "ussr", "post-soviet",
        # –°–í–û
        "svo", "—Å–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è", "special military operation",
        "–≤–æ–π–Ω–∞", "war", "conflict", "–∫–æ–Ω—Ñ–ª–∏–∫—Ç",
        "–Ω–∞—Å—Ç—É–ø–ª–µ–Ω–∏–µ", "offensive", "–∞—Ç–∞–∫–∞", "attack",
        "—É–¥–∞—Ä", "strike", "–æ–±—Å—Ç—Ä–µ–ª", "shelling",
        "–¥—Ä–æ–Ω", "drone", "missile", "—Ä–∞–∫–µ—Ç–∞",
        "—ç—Å–∫–∞–ª–∞—Ü–∏—è", "escalation", "–º–æ–±–∏–ª–∏–∑–∞—Ü–∏—è", "mobilization",
        "—Ñ—Ä–æ–Ω—Ç", "frontline", "–∑–∞—Ö–≤–∞—Ç", "capture",
        "–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ", "liberation", "–±–æ–π", "battle",
        "–ø–æ—Ç–µ—Ä–∏", "casualties", "–ø–æ–≥–∏–±", "killed",
        "—Ä–∞–Ω–µ–Ω", "injured", "–ø–ª–µ–Ω–Ω—ã–π", "prisoner of war",
        "–ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã", "talks", "–ø–µ—Ä–µ–º–∏—Ä–∏–µ", "ceasefire",
        "—Å–∞–Ω–∫—Ü–∏–∏", "sanctions", "–æ—Ä—É–∂–∏–µ", "weapons",
        "–ø–æ—Å—Ç–∞–≤–∫–∏", "supplies", "himars", "atacms",
        # –ö—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞
        "bitcoin", "btc", "–±–∏—Ç–∫–æ–∏–Ω", "ethereum", "eth",
        "binance coin", "bnb", "usdt", "tether",
        "xrp", "ripple", "cardano", "ada",
        "solana", "sol", "doge", "dogecoin",
        "avalanche", "avax", "polkadot", "dot",
        "chainlink", "link", "tron", "trx",
        "cbdc", "central bank digital currency", "—Ü–∏—Ñ—Ä–æ–≤–æ–π —Ä—É–±–ª—å",
        "digital yuan", "euro digital", "defi", "–¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏–Ω–∞–Ω—Å—ã",
        "nft", "non-fungible token", "sec", "—Ü–± —Ä—Ñ",
        "—Ä–µ–≥—É–ª—è—Ü–∏—è", "regulation", "–∑–∞–ø—Ä–µ—Ç", "ban",
        "–º–∞–π–Ω–∏–Ω–≥", "mining", "halving", "—Ö–∞–ª–≤–∏–Ω–≥",
        "–≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å", "volatility", "crash", "–∫—Ä–∞—Ö",
    ]
    return any(kw in text for kw in keywords)

def is_generic(desc: str) -> bool:
    return any(phrase in desc.lower() for phrase in ["appeared first", "read more", "¬©", "all rights"])

def is_article_sent(url: str) -> bool:
    try:
        resp = supabase.table("published_articles").select("url").eq("url", url).execute()
        return len(resp.data) > 0
    except Exception as e:
        logger.error(f"Supabase check error: {e}")
        return False

def mark_article_sent(url: str, title: str):
    try:
        supabase.table("published_articles").insert({"url": url, "title": title}).execute()
        logger.info(f"‚úÖ Saved: {url}")
    except Exception as e:
        logger.error(f"Supabase insert error: {e}")

def send_to_telegram(prefix: str, title: str, lead: str, url: str):
    try:
        title_ru = translate(title)
        lead_ru = translate(lead)
        message = f"<b>{prefix}</b>: {title_ru}\n\n{lead_ru}\n\n–ò—Å—Ç–æ—á–Ω–∏–∫: {url}"

        for ch in CHANNEL_IDS:
            resp = requests.post(
                f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage",
                json={
                    "chat_id": ch,
                    "text": message,
                    "parse_mode": "HTML"
                },
                timeout=10
            )
            if resp.status_code == 200:
                logger.info(f"üì§ Sent: {title[:60]}...")
            else:
                logger.error(f"‚ùå TG error: {resp.status_code} {resp.text}")
    except Exception as e:
        logger.exception(f"Telegram send failed: {e}")

def fetch_and_process():
    logger.info("üì° Checking feeds...")
    for src in SOURCES:
        try:
            logger.info(f"Fetching feed from {src['name']} ({src['rss']})")
            feed = feedparser.parse(src["rss"])
            if not feed.entries:
                logger.warning(f"Feed from {src['name']} is empty or invalid.")
                continue

            for entry in feed.entries:
                url = entry.get("link", "").strip()
                if not url or is_article_sent(url):
                    continue

                title = entry.get("title", "").strip()
                desc = (entry.get("summary") or entry.get("description") or "").strip()
                desc = clean_html(desc)
                if not title or not desc or is_generic(desc):
                    continue

                if not is_relevant(title, desc):
                    continue

                lead = desc.split("\n")[0].split(". ")[0].strip()
                if not lead:
                    continue

                send_to_telegram(src["name"], title, lead, url)
                mark_article_sent(url, title)
                time.sleep(0.5)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –æ—Ç–ø—Ä–∞–≤–∫–∞–º–∏

        except requests.exceptions.RequestException as e:
            logger.error(f"Network error fetching feed from {src['name']}: {e}")
        except Exception as e:
            logger.error(f"Error processing feed from {src['name']}: {e}")

    logger.info("‚úÖ Feed check completed.")

# === –ó–∞–ø—É—Å–∫ ===
if __name__ == "__main__":
    logger.info("üöÄ Starting Russia Monitor Bot (Background Worker)...")
    while True:
        fetch_and_process()
        logger.info("üí§ Sleeping for 10 minutes...")  # –ò–∑–º–µ–Ω–µ–Ω–æ –Ω–∞ 10 –º–∏–Ω—É—Ç
        time.sleep(10 * 60)  # –°–ø–∏–º 10 –º–∏–Ω—É—Ç –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π
